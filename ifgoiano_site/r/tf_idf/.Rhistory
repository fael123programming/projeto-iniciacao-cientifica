data <- c(836962, 670005, 760702, 418543, 305993, 586022, 439392, 806735, 789441, 805297, 693606, 641947, 731631, 762916, 687297, 577964, 608574, 338189, 742702, 740253, 414602, 422863, 842306, 796430, 783221, 410343, 507054)
data
data
quantile(data, prob = c(.25, .5, .75), type = 1)
data <- c(466565, 533097, 662830, 747738, 538861, 785591, 732920, 516169, 381282, 332191, 650453, 511281, 512419, 407361, 629718, 496882, 687915, 466148, 658433, 330061, 602968, 695330, 400290, 877885, 450114, 803743, 563465, 545334, 630502, 740911, 616578, 536530, 730177, 647438, 602675, 488436, 644958, 743645, 746834, 606122, 640365, 431670, 453651, 581547, 736512, 588121, 425169, 484183, 518946, 326952, 304295, 507567, 560948, 413374, 609377, 318756, 387983, 669662, 559285, 713625, 470320, 486861, 597232, 587713, 395933, 797989, 876572, 575586, 879872, 694684, 766020, 692250, 707191, 873597, 344292, 564383, 525518, 806957, 669805, 829583, 330544, 670811, 742504, 591918, 852958, 788451, 650196, 435672, 413123, 585054, 424806, 691486, 708922, 592543, 372976, 692536, 630824, 569753, 724519, 718326, 402952, 794537, 503779, 675800, 386097, 809345, 662112, 468372, 474019, 671663, 464763, 814395, 547938, 890146, 472567, 557276, 578567, 790490, 595460, 330056, 734208, 480924, 552863, 82182
2, 686318)
data
quantile(data, prob = c(.25, .5, .75), type = 1)
quantile(data, prob = c(.25, .5, .75), type = 1)
quantile(data, prob = c(.25, .5, .75), type = 1)
4/10
4//10
q()
setwd("~/projeto_iniciacao_cientifica/ifgoiano_site/r/tf_idf")
data.raw <- read.csv("../pubs_text_2023_02_13_23_22_05_4_05_04.csv")
length(which(!complete.cases(data.raw)))
?tokens
# install.packages(c("ggplot2", "e1071", "caret", "quanteda"))
library(quanteda, ggplot2)
?tokens
?tokens_ngrams
data.raw <- read.csv("../pubs_text_2023_02_13_23_22_05_4_05_04.csv")
data.tokens <- tokens(data.raw$text, what = "word", remove_numbers = TRUE,
remove_punct = TRUE, remove_symbols = TRUE,
remove_separators = TRUE)
data.tokens <- tokens_ngrams(data.tokens, n = 2)
data.tokens <- tokens_tolower(data.tokens)
data.tokens <- tokens_select(data.tokens, stopwords(language = "pt"), selection = "remove")
data.tokens <- tokens_wordstem(data.tokens, language = "portuguese")
data.dfm <- dfm(data.tokens, tolower = FALSE)
data.matrix <- as.matrix(data.dfm)
View(data.matrix)
View(data.matrix[1:50,])
data.raw <- read.csv("../pubs_text_2023_02_13_23_22_05_4_05_04.csv")
data.tokens <- tokens(data.raw$text, what = "word", remove_numbers = TRUE,
remove_punct = TRUE, remove_symbols = TRUE,
remove_separators = TRUE)
data.tokens <- tokens_ngrams(data.tokens, n = 2)
data.tokens <- tokens_tolower(data.tokens)
data.tokens <- tokens_select(data.tokens, stopwords(language = "pt"), selection = "remove")
data.dfm <- dfm(data.tokens, tolower = FALSE)
data.matrix <- as.matrix(data.dfm)
View(data.matrix[1:50,])
data.df <- cbind(text = data.raw$text, convert(data.dfm, to = "data.frame"))
names(data.df) <- make.names(names(data.df))
term.frequency <- function(row) {
row / sum(row)
}
inverse.doc.freq <- function(col) {
corpus.size <- length(col)
doc.count <- length(which(col > 0))
log10(corpus.size / doc.count)
}
tf.idf <- function(x, idf) {
x * idf
}
data.df <- apply(data.matrix, 1, term.frequency)
data.idf <- apply(data.matrix, 2, inverse.doc.freq)
data.tfidf <- apply(data.df, 2, tf.idf, idf = data.idf)
data.tfidf <- t(data.tfidf)
incomplete.cases <- which(!complete.cases(data.tfidf))
data.raw$text[incomplete.cases]
data.tfidf[incomplete.cases] <- rep(0.0, ncol(data.tfidf))
sum(which(!complete.cases(data.tfidf)))
data.dtframe <- cbind(text = data.raw$text, data.frame(data.tfidf))
View(data.dtframe[1:10, ])
View(data.dtframe[1:10, ])
write.csv(data.dtframe, "bigrams-tf-idf.csv")
result.df <- data.frame(text = data.dtframe$text)  # Copy the 'text' column to the new data frame
result.df$term <- ""
result.df$tfidf <- 0.0
tot.cols <- ncol(data.dtframe)
name.cols <- names(data.dtframe)
for (i in 1:nrow(result.df)) {
the.max <- which.max(data.dtframe[i, 2:tot.cols])
result.df$term[i] <- name.cols[the.max + 1]
result.df$tfidf[i] <- data.dtframe[i, the.max + 1]
}
write.csv(result.df, "bigrams-max-tf-idf-term.csv")
name.cols
max.values <- 10
term.counts <- table(result.df[["term"]])
sorted.counts <- sort(term.counts, decreasing = TRUE)
top.terms <- names(sorted.counts)[1:max.values]
top.counts <- sorted.counts[1:max.values]
counts.dtframe <- data.frame(term = top.terms, count = top.counts)
counts.dtframe <- subset(counts.dtframe, select = -count.Var1)
# library(ggplot2)
ggplot(counts.dtframe, aes(x = term, y = count.Freq, fill = term)) +
geom_bar(stat = "identity") +
labs(title = "Count of each term occurrence as max tf-idf",
x = "term", y = "count.Freq")
# install.packages(c("ggplot2", "e1071", "caret", "quanteda"))
library(c(quanteda, ggplot2))
# install.packages(c("ggplot2", "e1071", "caret", "quanteda"))
library(quanteda)
library(ggplot2)
r
# library(ggplot2)
ggplot(counts.dtframe, aes(x = term, y = count.Freq, fill = term)) +
geom_bar(stat = "identity") +
labs(title = "Count of each term occurrence as max tf-idf",
x = "term", y = "count.Freq")
library(topicmodels)
library(tidytext)
library(ggplot2)
library(dplyr)
library(tidyr)
library(quanteda)
library(tm)
delete.words <- c(
"dia", "dias", "ser", "podem", "confira", "final", "rio", "goiano", "verde",
"janeiro", "fevereiro", "marco", "março", "abril", "maio", "junho", "julho",
"agosto", "setembro", "outubro", "novembro", "dezembro", "mês", "ano", "sobre",
"hidrolândia", "morrinhos", "servidores", "reitoria", "realizado", "realiza",
"realizados", "realizadas", "lançado", "nacional", "ept", "lista", "ciclo",
"prorrogadas", "faz", "horas", "devem", "veja")
data.raw <- read.csv("pubs_text_2023_02_13_23_22_05_4_05_04.csv")
data.raw <- read.csv("../pubs_text_2023_02_13_23_22_05_4_05_04.csv")
data.tokens <- tokens(data.raw$text, what = "word", remove_numbers = TRUE,
remove_punct = TRUE, remove_symbols = TRUE,
remove_separators = TRUE)
data.tokens <- tokens_ngrams(data.tokens, n = 2)
# install.packages(c("ggplot2", "e1071", "caret", "quanteda"))
library(quanteda)
library(ggplot2)
data.raw <- read.csv("../pubs_text_2023_02_13_23_22_05_4_05_04.csv")
length(which(!complete.cases(data.raw)))  # See how many incomplete rows.
data.tokens <- tokens(data.raw$text, what = "word", remove_numbers = TRUE,
remove_punct = TRUE, remove_symbols = TRUE,
remove_separators = TRUE)
data.tokens <- tokens_tolower(data.tokens)
data.tokens <- tokens_select(data.tokens, stopwords(language = "pt"), selection = "remove")
data.tokens <- tokens_select(data.tokens, stopwords(language = "pt"), selection = "remove")
data.tokens <- tokens_select(data.tokens, stopwords(language = "portuguese"), selection = "remove")
data.tokens <- tokens_select(data.tokens, stopwords(kind = "pt"), selection = "remove")
stopwords(kind = "pt")
data.tokens <- tokens_ngrams(data.tokens, n = 2)
data.dfm <- dfm(data.tokens, tolower = FALSE)
data.matrix <- as.matrix(data.dfm)
data.df <- cbind(text = data.raw$text, convert(data.dfm, to = "data.frame"))
names(data.df) <- make.names(names(data.df))
term.frequency <- function(row) {
row / sum(row)
}
inverse.doc.freq <- function(col) {
corpus.size <- length(col)
doc.count <- length(which(col > 0))
log10(corpus.size / doc.count)
}
tf.idf <- function(x, idf) {
x * idf
}
data.df <- apply(data.matrix, 1, term.frequency)
data.idf <- apply(data.matrix, 2, inverse.doc.freq)
data.tfidf <- apply(data.df, 2, tf.idf, idf = data.idf)
data.tfidf <- t(data.tfidf)
incomplete.cases <- which(!complete.cases(data.tfidf))
data.raw$text[incomplete.cases]
data.tfidf[incomplete.cases] <- rep(0.0, ncol(data.tfidf))
sum(which(!complete.cases(data.tfidf)))
data.dtframe <- cbind(text = data.raw$text, data.frame(data.tfidf))
write.csv(data.dtframe, "bigrams-tf-idf.csv")
result.df <- data.frame(text = data.dtframe$text)  # Copy the 'text' column to the new data frame
result.df$term <- ""
result.df$tfidf <- 0.0
tot.cols <- ncol(data.dtframe)
name.cols <- names(data.dtframe)
for (i in 1:nrow(result.df)) {
the.max <- which.max(data.dtframe[i, 2:tot.cols])
result.df$term[i] <- name.cols[the.max + 1]
result.df$tfidf[i] <- data.dtframe[i, the.max + 1]
}
write.csv(result.df, "bigrams-max-tf-idf-term.csv")
max.values <- 10
term.counts <- table(result.df[["term"]])
sorted.counts <- sort(term.counts, decreasing = TRUE)
top.terms <- names(sorted.counts)[1:max.values]
top.counts <- sorted.counts[1:max.values]
counts.dtframe <- data.frame(term = top.terms, count = top.counts)
counts.dtframe <- subset(counts.dtframe, select = -count.Var1)
ggplot(counts.dtframe, aes(x = term, y = count.Freq, fill = term)) +
geom_bar(stat = "identity") +
labs(title = "Count of each term occurrence as max tf-idf",
x = "term", y = "count.Freq")
library(topicmodels)
library(tidytext)
library(ggplot2)
library(dplyr)
library(tidyr)
library(quanteda)
library(tm)
delete.words <- c(
"dia", "dias", "ser", "podem", "confira", "final", "rio", "goiano", "verde",
"janeiro", "fevereiro", "marco", "março", "abril", "maio", "junho", "julho",
"agosto", "setembro", "outubro", "novembro", "dezembro", "mês", "ano", "sobre",
"hidrolândia", "morrinhos", "servidores", "reitoria", "realizado", "realiza",
"realizados", "realizadas", "lançado", "nacional", "ept", "lista", "ciclo",
"prorrogadas", "faz", "horas", "devem", "veja")
data.raw <- read.csv("../pubs_text_2023_02_13_23_22_05_4_05_04.csv")
data.tokens <- tokens(data.raw$text, what = "word", remove_numbers = TRUE,
remove_punct = TRUE, remove_symbols = TRUE,
remove_separators = TRUE)
data.tokens <- tokens_tolower(data.tokens)
data.tokens <- tokens_select(data.tokens, stopwords(kind = "pt"), selection = "remove")
data.tokens <- tokens_ngrams(data.tokens, n = 2)
data.tokens <- tokens_select(data.tokens, delete.words, selection = "remove")
# data.tokens <- tokens_wordstem(data.tokens, language = "portuguese")
data.dtm <- DocumentTermMatrix(Corpus(VectorSource(data.tokens)))
data.dtm <- data.dtm[1:length(data.tokens),]  # Cut off the rows beyond the size of the dataset.
data.dtm <- data.dtm[-which(apply(as.matrix(data.dtm), 1, function(row) all(row == 0)) == TRUE), ]  # Cut off the rows with only zeros.
data.lda <- LDA(data.dtm, k = 8, control = list(seed = 1234))
data.td <- tidy(data.lda)
data.top.terms <- data.td %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
data.top.terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
1+1
View(data.td[1:10,])
View(data.tokens[1:10,])
data.raw <- read.csv("../pubs_text_2023_02_13_23_22_05_4_05_04.csv")
data.tokens <- tokens(data.raw$text, what = "word", remove_numbers = TRUE,
remove_punct = TRUE, remove_symbols = TRUE,
remove_separators = TRUE)
data.tokens <- tokens_tolower(data.tokens)
data.tokens <- tokens_select(data.tokens, stopwords(kind = "pt"), selection = "remove")
data.tokens <- tokens_select(data.tokens, delete.words, selection = "remove")
data.tokens <- tokens_ngrams(data.tokens, n = 2)
data.dtm <- DocumentTermMatrix(Corpus(VectorSource(data.tokens)))
data.dtm <- data.dtm[1:length(data.tokens),]  # Cut off the rows beyond the size of the dataset.
data.dtm <- data.dtm[-which(apply(as.matrix(data.dtm), 1, function(row) all(row == 0)) == TRUE), ]  # Cut off the rows with only zeros.
data.lda <- LDA(data.dtm, k = 2, control = list(seed = 1234))
data.td <- tidy(data.lda)
View(data.td)
data.top.terms <- data.td %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
data.top.terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
data.top.terms <- data.td %>%
group_by(topic) %>%
slice_max(beta, n = 1) %>%
ungroup() %>%
arrange(topic, -beta)
data.top.terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
View(data.top.terms[1:10,])
gc()
